{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YpiM19rwhP2",
    "outputId": "7464d4ba-5bd5-433f-f1ef-cde9f1e252b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: LughaatNLP in /usr/local/lib/python3.10/dist-packages (1.0.6)\n",
      "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (1.13.1)\n",
      "Requirement already satisfied: gtts in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (2.5.3)\n",
      "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (3.10.4)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from LughaatNLP) (0.25.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts->LughaatNLP) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts->LughaatNLP) (8.1.7)\n",
      "Requirement already satisfied: Levenshtein==0.26.0 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein->LughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.0->python-Levenshtein->LughaatNLP) (3.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->LughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->LughaatNLP) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition->LughaatNLP) (4.12.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->LughaatNLP) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->LughaatNLP) (0.44.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->LughaatNLP) (13.9.2)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->LughaatNLP) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->LughaatNLP) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts->LughaatNLP) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts->LughaatNLP) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->LughaatNLP) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->LughaatNLP) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->LughaatNLP) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->LughaatNLP) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->LughaatNLP) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->LughaatNLP) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->LughaatNLP) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install  LughaatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GL2vU49AYMm",
    "outputId": "2f98d696-a4e6-462e-d8e5-b60aab72fdb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22438, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_excel('cleaned_parallel-corpus.xlsx')\n",
    "# Filter out rows with less than 3 words in either English or Urdu\n",
    "data = data[data['SENTENCES'].str.split().str.len() >= 3]\n",
    "data = data[data['MEANING'].str.split().str.len() >= 3]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wm92kKvnBHGS",
    "outputId": "a2ea6cdb-0e5b-4555-b06b-f9163eccf173"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-4fde82b1486b>:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['SENTENCES'] = data['SENTENCES'].apply(normalize_string)\n",
      "<ipython-input-39-4fde82b1486b>:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['English_tokens'] = data['SENTENCES'].apply(tokenize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 8351\n",
      "Urdu Vocabulary Size: 8469\n",
      "Training samples: 13147\n",
      "Validation samples: 1461\n",
      "Test samples: 1624\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('cleaned_parallel-corpus.xlsx')\n",
    "# Filter out rows with less than 3 words in either English or Urdu\n",
    "data = data[data['SENTENCES'].str.split().str.len() >= 3]\n",
    "data = data[data['MEANING'].str.split().str.len() >= 3]\n",
    "\n",
    "data = data[data['SENTENCES'].str.split().str.len() <= 20]\n",
    "data = data[data['MEANING'].str.split().str.len() <= 20]\n",
    "# Data Preprocessing\n",
    "def normalize_string(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters.\"\"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"Tokenize a sentence into words.\"\"\"\n",
    "    return sentence.strip().split()\n",
    "\n",
    "# Preprocess English sentences\n",
    "data['SENTENCES'] = data['SENTENCES'].apply(normalize_string)\n",
    "data['English_tokens'] = data['SENTENCES'].apply(tokenize)\n",
    "\n",
    "   return tokens\n",
    "\n",
    "data['Urdu_tokens'] = data['MEANING'].apply(urdu_text_processing.urdu_tokenize)\n",
    "\n",
    "# Build Vocabulary\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    \"\"\"Build vocabulary with word frequencies above min_freq.\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        word_counts.update(sentence)\n",
    "    vocab = ['<pad>', '<sos>', '<eos>', '<unk>']  # Special tokens\n",
    "    vocab += [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "eng_word2idx, eng_idx2word = build_vocab(data['English_tokens'], min_freq=1)\n",
    "urd_word2idx, urd_idx2word = build_vocab(data['Urdu_tokens'], min_freq=1)\n",
    "\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "urd_vocab_size = len(urd_word2idx)\n",
    "\n",
    "print(\"English Vocabulary Size:\", eng_vocab_size)\n",
    "print(\"Urdu Vocabulary Size:\", urd_vocab_size)\n",
    "\n",
    "# Convert sentences to indices\n",
    "def sentence_to_indices(sentence, word2idx):\n",
    "    \"\"\"Convert a tokenized sentence to indices, adding <sos> and <eos> tokens.\"\"\"\n",
    "    return [word2idx.get('<sos>')] + \\\n",
    "           [word2idx.get(word, word2idx.get('<unk>')) for word in sentence] + \\\n",
    "           [word2idx.get('<eos>')]\n",
    "\n",
    "data['English_indices'] = data['English_tokens'].apply(lambda x: sentence_to_indices(x, eng_word2idx))\n",
    "data['Urdu_indices'] = data['Urdu_tokens'].apply(lambda x: sentence_to_indices(x, urd_word2idx))\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng_seq = self.data.loc[idx, 'English_indices']\n",
    "        urd_seq = self.data.loc[idx, 'Urdu_indices']\n",
    "        return torch.tensor(eng_seq, dtype=torch.long), torch.tensor(urd_seq, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences to the same length.\"\"\"\n",
    "    eng_seqs, urd_seqs = zip(*batch)\n",
    "    eng_lens = [len(seq) for seq in eng_seqs]\n",
    "    urd_lens = [len(seq) for seq in urd_seqs]\n",
    "    eng_seqs_padded = nn.utils.rnn.pad_sequence(eng_seqs, padding_value=eng_word2idx['<pad>'], batch_first=True)\n",
    "    urd_seqs_padded = nn.utils.rnn.pad_sequence(urd_seqs, padding_value=urd_word2idx['<pad>'], batch_first=True)\n",
    "    return eng_seqs_padded, urd_seqs_padded, eng_lens, urd_lens\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = TranslationDataset(train_data)\n",
    "val_dataset = TranslationDataset(val_data)\n",
    "test_dataset = TranslationDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from LughaatNLP import LughaatNLP\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture with Attention using RNN\n",
    "class Seq2SeqRNN_Attention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device='cuda'):\n",
    "        super(Seq2SeqRNN_Attention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, src_lengths, trg_lengths, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        max_trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
    "\n",
    "        input = trg[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, max_trg_len):\n",
    "            output, hidden = self.decoder(input.unsqueeze(1), hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "# Instantiate the models with hyperparameter tuning\n",
    "embedding_dim = 512\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 3\n",
    "\n",
    "# CUDA configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the models with hyperparameter tuning\n",
    "encoder = EncoderRNN(eng_vocab_size, hidden_size, embedding_dim, num_layers).to(device)\n",
    "decoder = DecoderRNN_Attention(hidden_size, urd_vocab_size, embedding_dim, num_layers).to(device)\n",
    "model = Seq2SeqRNN_Attention(encoder, decoder, device=device).to(device)\n",
    "\n",
    "# Training Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=urd_word2idx['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QV7F_8sIwvOU",
    "outputId": "2a28eb70-819b-4b6d-d7fb-f4e0cdd820ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/3 | Train Loss: 5.382 | Val Loss: 5.692\n",
      "Epoch 2/3 | Train Loss: 4.869 | Val Loss: 5.640\n",
      "Epoch 3/3 | Train Loss: 4.551 | Val Loss: 5.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-6e5bdd3ee6c8>:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('seq2seq-rnn-attention-best.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU Score (RNN with Attention): 6.30\n",
      "English: How can I communicate with my parents?\n",
      "Urdu Translation (RNN with Attention): کیا اپنے والدین کو اپنے والدین کے ساتھ مارسیلز\n",
      "\n",
      "English: If you’ve asked yourself such questions, you’re not alone.\n",
      "Urdu Translation (RNN with Attention): تم تم آپ کو دوبارہ سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا سامنا\n",
      "\n",
      "English: Thumbs up\n",
      "Urdu Translation (RNN with Attention): بدترین ایلن کو بہتر\n",
      "\n",
      "English: Food was good but service was very slow.\n",
      "Urdu Translation (RNN with Attention): کھانا اچھا تھا لیکن سروس تھوڑی سست تھی ۔\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modified training loop with model saving after each epoch\n",
    "def train_model(model, iterator, optimizer, criterion, clip=1, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg, src_lengths, trg_lengths) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, src_lengths, trg_lengths, teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_model(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg, src_lengths, trg_lengths) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, src_lengths, trg_lengths, teacher_forcing_ratio=0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training loop with saving model at each epoch\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tf_ratio = max(0.5 - epoch * 0.02, 0)  # Gradually decrease teacher forcing ratio\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, teacher_forcing_ratio=tf_ratio)\n",
    "    valid_loss = evaluate_model(model, val_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    # Save model after each epoch\n",
    "    torch.save(model.state_dict(), f'seq2seq-rnn-attention-epoch{epoch+1}.pt')\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq-rnn-attention-best.pt')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "# Load the best model for inference\n",
    "model.load_state_dict(torch.load('seq2seq-rnn-attention-best.pt'))\n",
    "\n",
    "# Evaluation\n",
    "def translate_sentence(model, sentence, eng_word2idx, urd_idx2word, max_length=50):\n",
    "    model.eval()\n",
    "    tokens = tokenize(normalize_string(sentence))\n",
    "    indices = sentence_to_indices(tokens, eng_word2idx)\n",
    "    # Move src_tensor to the same device as the model\n",
    "    src_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    src_length = [len(indices)]\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_length)\n",
    "    trg_indices = [urd_word2idx['<sos>']]\n",
    "    for i in range(max_length):\n",
    "        # Move trg_tensor to the same device as the model\n",
    "        trg_tensor = torch.tensor([trg_indices[-1]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor.unsqueeze(1), hidden, encoder_outputs)\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indices.append(pred_token)\n",
    "            if pred_token == urd_word2idx['<eos>']:\n",
    "                break\n",
    "    trg_tokens = [urd_idx2word.get(idx, '<unk>') for idx in trg_indices]\n",
    "    return ' '.join(trg_tokens[1:-1])  # Exclude <sos> and <eos>\n",
    "\n",
    "def calculate_bleu(model, data, eng_word2idx, urd_idx2word):\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    for idx in range(len(data)):\n",
    "        src_tokens = data.iloc[idx]['English_tokens']\n",
    "        trg_tokens = data.iloc[idx]['Urdu_tokens']\n",
    "        src_sentence = ' '.join(src_tokens)\n",
    "        reference = [' '.join(trg_tokens)]\n",
    "        hypothesis = translate_sentence(model, src_sentence, eng_word2idx, urd_idx2word)\n",
    "        references.append([reference[0].split()])\n",
    "        hypotheses.append(hypothesis.split())\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score\n",
    "\n",
    "bleu_score = calculate_bleu(model, test_data, eng_word2idx, urd_idx2word)\n",
    "print(f'Test BLEU Score (RNN with Attention): {bleu_score*100:.2f}')\n",
    "\n",
    "# Example Translations\n",
    "test_sentences = [\n",
    "    \"How can I communicate with my parents?\",\n",
    "    \"If you’ve asked yourself such questions, you’re not alone.\",\n",
    "    \"Thumbs up\",\n",
    "    \"Food was good but service was very slow.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation = translate_sentence(model, sentence, eng_word2idx, urd_idx2word)\n",
    "    print(f'English: {sentence}')\n",
    "    print(f'Urdu Translation (RNN with Attention): {translation}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivw3s_bgy0Xp"
   },
   "source": [
    "# LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHVDPb4US6EK",
    "outputId": "c61489ad-44b0-4c14-b1c4-445083d2c150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/5 | Train Loss: 4.660 | Val Loss: 4.504\n",
      "Epoch 2/5 | Train Loss: 3.294 | Val Loss: 4.144\n",
      "Epoch 3/5 | Train Loss: 2.358 | Val Loss: 4.073\n",
      "Epoch 4/5 | Train Loss: 1.751 | Val Loss: 4.132\n",
      "Epoch 5/5 | Train Loss: 1.419 | Val Loss: 4.166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-568ad724f77c>:177: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('seq2seq-lstm-attention-best.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU Score (LSTM with Attention): 18.64\n",
      "English: How can I communicate with my parents?\n",
      "Urdu Translation (LSTM with Attention): میں اپنے والدین سے کیسے کیسے کیسے کیسے ؟ ؟\n",
      "\n",
      "English: If you’ve asked yourself such questions, you’re not alone.\n",
      "Urdu Translation (LSTM with Attention): اگر آپ ایسا نہیں کہ تم نے آپ کو ایسا نہیں نہیں ہیں ۔\n",
      "\n",
      "English: Thumbs up\n",
      "Urdu Translation (LSTM with Attention): بہت خوب\n",
      "\n",
      "English: Food was good but service was very slow.\n",
      "Urdu Translation (LSTM with Attention): کھانا اچھا تھا لیکن سروس بہت سست تھی ۔\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model Architecture with Attention using LSTM\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dim, num_layers=1):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, (hidden, cell) = self.lstm(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs, (hidden, cell)  # outputs: (batch_size, seq_length, hidden_size)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        timestep = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, timestep, 1)  # (batch_size, seq_len, hidden_size)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, seq_len, hidden_size)\n",
    "        energy = energy.transpose(1, 2)  # (batch_size, hidden_size, seq_len)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        energy = torch.bmm(v, energy)  # (batch_size, 1, seq_len)\n",
    "        attention_weights = F.softmax(energy.squeeze(1), dim=1)  # (batch_size, seq_len)\n",
    "        return attention_weights\n",
    "\n",
    "class DecoderLSTM_Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embedding_dim, num_layers=1):\n",
    "        super(DecoderLSTM_Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        # x: (batch_size, 1)\n",
    "        embedded = self.embedding(x)  # (batch_size, 1, embedding_dim)\n",
    "        h_n, c_n = hidden\n",
    "        # Use the last layer's hidden state\n",
    "        decoder_hidden = h_n[-1]  # (batch_size, hidden_size)\n",
    "        attention_weights = self.attention(decoder_hidden, encoder_outputs)  # (batch_size, seq_len)\n",
    "        attention_weights = attention_weights.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "        context = torch.bmm(attention_weights, encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)  # (batch_size, 1, embedding_dim + hidden_size)\n",
    "        output, (h_n, c_n) = self.lstm(lstm_input, (h_n, c_n))\n",
    "        output = torch.cat((output.squeeze(1), context.squeeze(1)), dim=1)  # (batch_size, hidden_size * 2)\n",
    "        prediction = self.fc(output)  # (batch_size, output_size)\n",
    "        return prediction, (h_n, c_n)\n",
    "\n",
    "class Seq2SeqLSTM_Attention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device='cuda'):\n",
    "        super(Seq2SeqLSTM_Attention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, src_lengths, trg_lengths, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        max_trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src, src_lengths)\n",
    "\n",
    "        input = trg[:, 0]  # <sos>\n",
    "        hidden = (hidden, cell)  # Prepare hidden state for decoder\n",
    "\n",
    "        for t in range(1, max_trg_len):\n",
    "            output, hidden = self.decoder(input.unsqueeze(1), hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "# Instantiate the models with hyperparameter tuning\n",
    "embedding_dim = 512\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 5\n",
    "\n",
    "# CUDA configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the models with hyperparameter tuning\n",
    "encoder = EncoderLSTM(eng_vocab_size, hidden_size, embedding_dim, num_layers).to(device)\n",
    "decoder = DecoderLSTM_Attention(hidden_size, urd_vocab_size, embedding_dim, num_layers).to(device)\n",
    "model = Seq2SeqLSTM_Attention(encoder, decoder, device=device).to(device)\n",
    "\n",
    "# Training Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=urd_word2idx['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Modified training loop with model saving after each epoch\n",
    "def train_model(model, iterator, optimizer, criterion, clip=1, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg, src_lengths, trg_lengths) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, src_lengths, trg_lengths, teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_model(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg, src_lengths, trg_lengths) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, src_lengths, trg_lengths, teacher_forcing_ratio=0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training loop with saving model at each epoch\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tf_ratio = max(0.5 - epoch * 0.02, 0)  # Gradually decrease teacher forcing ratio\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, teacher_forcing_ratio=tf_ratio)\n",
    "    valid_loss = evaluate_model(model, val_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    # Save model after each epoch\n",
    "    torch.save(model.state_dict(), f'seq2seq-lstm-attention-epoch{epoch+1}.pt')\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq-lstm-attention-best.pt')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('seq2seq-lstm-attention-best.pt'))\n",
    "\n",
    "# Evaluation\n",
    "def translate_sentence(model, sentence, eng_word2idx, urd_idx2word, max_length=50):\n",
    "    model.eval()\n",
    "    tokens = tokenize(normalize_string(sentence))\n",
    "    indices = sentence_to_indices(tokens, eng_word2idx)\n",
    "    src_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    src_length = [len(indices)]\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor, src_length)\n",
    "    hidden = (hidden, cell)\n",
    "    trg_indices = [urd_word2idx['<sos>']]\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.tensor([trg_indices[-1]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor.unsqueeze(1), hidden, encoder_outputs)\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indices.append(pred_token)\n",
    "            if pred_token == urd_word2idx['<eos>']:\n",
    "                break\n",
    "    trg_tokens = [urd_idx2word.get(idx, '<unk>') for idx in trg_indices]\n",
    "    return ' '.join(trg_tokens[1:-1])  # Exclude <sos> and <eos>\n",
    "\n",
    "def calculate_bleu(model, data, eng_word2idx, urd_idx2word):\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    for idx in range(len(data)):\n",
    "        src_tokens = data.iloc[idx]['English_tokens']\n",
    "        trg_tokens = data.iloc[idx]['Urdu_tokens']\n",
    "        src_sentence = ' '.join(src_tokens)\n",
    "        reference = [' '.join(trg_tokens)]\n",
    "        hypothesis = translate_sentence(model, src_sentence, eng_word2idx, urd_idx2word)\n",
    "        references.append([reference[0].split()])\n",
    "        hypotheses.append(hypothesis.split())\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score\n",
    "\n",
    "bleu_score = calculate_bleu(model, test_data, eng_word2idx, urd_idx2word)\n",
    "print(f'Test BLEU Score (LSTM with Attention): {bleu_score*100:.2f}')\n",
    "\n",
    "# Example Translations\n",
    "test_sentences = [\n",
    "    \"How can I communicate with my parents?\",\n",
    "    \"If you’ve asked yourself such questions, you’re not alone.\",\n",
    "    \"Thumbs up\",\n",
    "    \"Food was good but service was very slow.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation = translate_sentence(model, sentence, eng_word2idx, urd_idx2word)\n",
    "    print(f'English: {sentence}')\n",
    "    print(f'Urdu Translation (LSTM with Attention): {translation}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tGej7ZoTTM7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
